"""
AIæœåŠ¡æ¨¡å—
ä¼˜å…ˆä½¿ç”¨ç«å±±å¼•æ“APIï¼Œé›†æˆRAGç³»ç»Ÿ
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
import numpy as np
from pydantic import BaseModel

import chromadb
from chromadb.config import Settings

from .volcengine_client import volcengine_client
from ..core.model_config import model_manager

# é…ç½®æ—¥å¿—
logger.add("logs/ai_service.log", rotation="1 day", retention="7 days")

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str  # "user" æˆ– "assistant"
    content: str
    timestamp: Optional[str] = None

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    content: str
    sources: List[Dict[str, Any]] = []
    model: str
    usage: Dict[str, int] = {}

class AIService:
    """AIæœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        self.vector_db = None
        self.initialize_models()
        self.initialize_vector_db()
    
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨ç«å±±å¼•æ“"""
        try:
            # æµ‹è¯•ç«å±±å¼•æ“è¿æ¥
            if volcengine_client.test_connection():
                logger.info("âœ… ç«å±±å¼•æ“æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ ç«å±±å¼•æ“è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def initialize_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“ - æ”¯æŒå¤šäººåä½œçš„æŒä¹…åŒ–"""
        try:
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯ - ä½¿ç”¨æŒä¹…åŒ–é…ç½®
            chroma_client = chromadb.PersistentClient(
                path="../chroma_db",  # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„chroma_db
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True,
                    is_persistent=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ - æ”¯æŒå¤šäººåä½œ
            self.vector_db = chroma_client.get_or_create_collection(
                name="project_documents",
                metadata={
                    "hnsw:space": "cosine",
                    "hnsw:construction_ef": 100,
                    "hnsw:search_ef": 50,
                    "description": "AIé¡¹ç›®ç®¡ç†ç³»ç»Ÿæ–‡æ¡£å‘é‡æ•°æ®åº“",
                    "version": "1.0.0",
                    "created_by": "ai_project_manager",
                    "supports_multi_user": True
                }
            )
            
            # è®°å½•æ•°æ®åº“ä¿¡æ¯
            logger.info(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"  è·¯å¾„: ../chroma_db")
            logger.info(f"  é›†åˆ: project_documents")
            logger.info(f"  æ”¯æŒå¤šäººåä½œ: æ˜¯")
            
        except Exception as e:
            logger.error(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_db = None
    
    async def chat_completion(
        self, 
        messages: List[ChatMessage], 
        project_context: Optional[str] = None,
        stream: bool = False,
        model_name: Optional[str] = None
    ) -> ChatResponse:
        """èŠå¤©å®Œæˆ"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(project_context)
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            api_messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            for msg in messages:
                # å¤„ç†å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼çš„æ¶ˆæ¯
                if isinstance(msg, dict):
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                else:
                    role = getattr(msg, "role", "user")
                    content = getattr(msg, "content", "")
                    
                api_messages.append({
                    "role": role,
                    "content": content
                })
            
            # ä½¿ç”¨ç«å±±å¼•æ“ç”Ÿæˆå›å¤
            response_content = volcengine_client.chat_completion(api_messages)
            
            return ChatResponse(
                content=response_content,
                model=volcengine_client.llm_model,
                usage={
                    "prompt_tokens": len(str(api_messages)),
                    "completion_tokens": len(response_content),
                    "total_tokens": len(str(api_messages)) + len(response_content)
                }
            )
                
        except Exception as e:
            logger.error(f"èŠå¤©å®Œæˆå¤±è´¥: {e}")
            return ChatResponse(
                content="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                model="error",
                usage={}
            )
    
    async def chat_completion_stream(
        self, 
        messages: List[ChatMessage], 
        project_context: Optional[str] = None,
        model_name: Optional[str] = None
    ):
        """èŠå¤©å®Œæˆ - æµå¼å“åº”"""
        try:
            logger.info(f"ğŸ”¥ å¼€å§‹æµå¼èŠå¤©å®Œæˆï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
            logger.info(f"ğŸ”¥ æ¶ˆæ¯ç±»å‹: {[type(msg).__name__ for msg in messages]}")
            logger.info(f"ğŸ”¥ å‰3æ¡æ¶ˆæ¯å†…å®¹: {messages[:3] if len(messages) <= 3 else messages[:3]}")
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(project_context)
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            api_messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            for msg in messages:
                # å¤„ç†å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼çš„æ¶ˆæ¯
                if isinstance(msg, dict):
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                else:
                    role = getattr(msg, "role", "user")
                    content = getattr(msg, "content", "")
                    
                api_messages.append({
                    "role": role,
                    "content": content
                })
            
            # å°è¯•ä½¿ç”¨çœŸå®çš„æµå¼AIæœåŠ¡
            try:
                # è¿™é‡Œå¯ä»¥æ¥å…¥çœŸå®çš„æµå¼AIæœåŠ¡
                # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿçš„æµå¼å“åº”
                response_content = volcengine_client.chat_completion(api_messages)
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º - å°†å®Œæ•´å›å¤æŒ‰å­—ç¬¦åˆ†å‰²
                import asyncio
                words = response_content.split()
                for i, word in enumerate(words):
                    if i == 0:
                        yield word
                    else:
                        yield f" {word}"
                    await asyncio.sleep(0.1)  # æ§åˆ¶è¾“å‡ºé€Ÿåº¦
                    
            except Exception as ai_error:
                logger.warning(f"AIæœåŠ¡æµå¼è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {ai_error}")
                # é™çº§å¤„ç† - ç”Ÿæˆæ™ºèƒ½å›å¤å¹¶æµå¼è¾“å‡º
                last_message_content = ""
                if messages:
                    last_msg = messages[-1]
                    if isinstance(last_msg, dict):
                        last_message_content = last_msg.get("content", "")
                    else:
                        last_message_content = getattr(last_msg, "content", "")
                        
                fallback_response = self._generate_fallback_response(
                    last_message_content, 
                    project_context
                )
                
                words = fallback_response.split()
                for i, word in enumerate(words):
                    if i == 0:
                        yield word
                    else:
                        yield f" {word}"
                    await asyncio.sleep(0.08)  # ç¨å¿«ä¸€äº›çš„è¾“å‡ºé€Ÿåº¦
                    
        except Exception as e:
            logger.error(f"æµå¼èŠå¤©å®Œæˆå¤±è´¥: {e}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"å®Œæ•´å †æ ˆ: {traceback.format_exc()}")
            
            # å‘é€é”™è¯¯ä¿¡æ¯ - ä½†å®é™…ä¸Šåº”è¯¥å‘é€æ­£å¸¸å›å¤
            # ä¸´æ—¶ä¿®å¤ï¼šç›´æ¥å‘é€ä¸€ä¸ªå‹å¥½çš„å›å¤
            friendly_response = "ä½ å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ã€‚è™½ç„¶å½“å‰é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œä½†æˆ‘å¾ˆä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥ä¸ºæ‚¨åšçš„å—ï¼Ÿ"
            
            words = friendly_response.split()
            for i, word in enumerate(words):
                if i == 0:
                    yield word
                else:
                    yield f" {word}"
                import asyncio
                await asyncio.sleep(0.1)
    
    def _generate_fallback_response(self, user_input: str, project_context: Optional[str] = None) -> str:
        """ç”Ÿæˆé™çº§å›å¤"""
        if not user_input:
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"
        
        # æ ¹æ®å…³é”®è¯ç”Ÿæˆç›¸å…³å›å¤
        user_input_lower = user_input.lower()
        
        if any(keyword in user_input_lower for keyword in ['æ¶æ„', 'è®¾è®¡', 'ç³»ç»Ÿ']):
            return """å…³äºç³»ç»Ÿæ¶æ„è®¾è®¡ï¼Œæˆ‘å»ºè®®è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–¹é¢ï¼š

ğŸ—ï¸ **æ¶æ„è®¾è®¡åŸåˆ™ï¼š**
1. **æ¨¡å—åŒ–è®¾è®¡** - å°†ç³»ç»Ÿåˆ†è§£ä¸ºç‹¬ç«‹çš„æ¨¡å—
2. **å¯æ‰©å±•æ€§** - æ”¯æŒæœªæ¥åŠŸèƒ½æ‰©å±•
3. **é«˜å¯ç”¨æ€§** - ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
4. **æ€§èƒ½ä¼˜åŒ–** - å…³æ³¨å“åº”æ—¶é—´å’Œååé‡

ğŸ’¡ **æŠ€æœ¯é€‰å‹å»ºè®®ï¼š**
- é€‰æ‹©æˆç†Ÿç¨³å®šçš„æŠ€æœ¯æ ˆ
- è€ƒè™‘å›¢é˜ŸæŠ€æœ¯æ ˆç†Ÿæ‚‰åº¦
- è¯„ä¼°æŠ€æœ¯çš„é•¿æœŸç»´æŠ¤æ€§
- æƒè¡¡å¼€å‘æ•ˆç‡å’Œæ€§èƒ½éœ€æ±‚

æ‚¨å¸Œæœ›æ·±å…¥è®¨è®ºå“ªä¸ªå…·ä½“çš„æ¶æ„æ–¹é¢ï¼Ÿ"""

        elif any(keyword in user_input_lower for keyword in ['æ•°æ®', 'æ•°æ®åº“', 'å­˜å‚¨']):
            return """å…³äºæ•°æ®ç®¡ç†å’Œå­˜å‚¨ï¼Œè¿™é‡Œæœ‰ä¸€äº›ä¸“ä¸šå»ºè®®ï¼š

ğŸ“Š **æ•°æ®å­˜å‚¨ç­–ç•¥ï¼š**
1. **å…³ç³»å‹æ•°æ®åº“** - é€‚åˆç»“æ„åŒ–æ•°æ®å’Œäº‹åŠ¡å¤„ç†
2. **éå…³ç³»å‹æ•°æ®åº“** - é€‚åˆå¤§è§„æ¨¡æ•°æ®å’Œçµæ´»schema
3. **ç¼“å­˜ç³»ç»Ÿ** - æå‡æ•°æ®è®¿é—®æ€§èƒ½
4. **æ•°æ®å¤‡ä»½** - ç¡®ä¿æ•°æ®å®‰å…¨æ€§

ğŸ” **æ•°æ®å¤„ç†æœ€ä½³å®è·µï¼š**
- æ•°æ®æ¸…æ´—å’ŒéªŒè¯
- å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§
- å®æ–½æ•°æ®å®‰å…¨ç­–ç•¥
- ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½

éœ€è¦é’ˆå¯¹ç‰¹å®šçš„æ•°æ®åœºæ™¯è¿›è¡Œæ·±å…¥åˆ†æå—ï¼Ÿ"""

        elif any(keyword in user_input_lower for keyword in ['AI', 'æœºå™¨å­¦ä¹ ', 'ç®—æ³•', 'æ¨¡å‹']):
            return """å…³äºAIå’Œæœºå™¨å­¦ä¹ å®æ–½ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹æŒ‡å¯¼ï¼š

ğŸ¤– **AIé¡¹ç›®å¼€å‘æµç¨‹ï¼š**
1. **éœ€æ±‚åˆ†æ** - æ˜ç¡®AIåº”ç”¨åœºæ™¯
2. **æ•°æ®å‡†å¤‡** - æ”¶é›†å’Œé¢„å¤„ç†è®­ç»ƒæ•°æ®
3. **æ¨¡å‹é€‰æ‹©** - æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç®—æ³•
4. **æ¨¡å‹è®­ç»ƒ** - ä½¿ç”¨åˆé€‚çš„è®­ç»ƒç­–ç•¥
5. **æ¨¡å‹è¯„ä¼°** - å¤šç»´åº¦è¯„ä¼°æ¨¡å‹æ€§èƒ½
6. **éƒ¨ç½²ä¸Šçº¿** - å°†æ¨¡å‹é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒ

âš¡ **å…³é”®æŠ€æœ¯è¦ç‚¹ï¼š**
- ç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§
- è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„å¤„ç†
- æ¨¡å‹è§£é‡Šæ€§å’Œå¯ä¿¡åº¦
- æŒç»­å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°

æ‚¨æƒ³äº†è§£å“ªä¸ªå…·ä½“çš„AIæŠ€æœ¯ç»†èŠ‚ï¼Ÿ"""

        else:
            # é€šç”¨å›å¤
            context_info = f"åœ¨{project_context}çš„èƒŒæ™¯ä¸‹ï¼Œ" if project_context else ""
            return f"""æ„Ÿè°¢æ‚¨çš„æé—®ï¼{context_info}æˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹åˆ†æï¼š

ğŸ’¡ **é—®é¢˜ç†è§£ï¼š**
æ‚¨è¯¢é—®çš„"{user_input}"æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚

ğŸ¯ **å»ºè®®æ–¹å‘ï¼š**
1. **æ·±å…¥åˆ†æ** - ä»å¤šä¸ªè§’åº¦ç†è§£é—®é¢˜æœ¬è´¨
2. **æœ€ä½³å®è·µ** - å‚è€ƒè¡Œä¸šæ ‡å‡†å’ŒæˆåŠŸæ¡ˆä¾‹
3. **é£é™©è¯„ä¼°** - è¯†åˆ«æ½œåœ¨çš„æŠ€æœ¯å’Œä¸šåŠ¡é£é™©
4. **å®æ–½ç­–ç•¥** - åˆ¶å®šå¾ªåºæ¸è¿›çš„è§£å†³æ–¹æ¡ˆ

ğŸš€ **ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š**
- æ”¶é›†æ›´å¤šè¯¦ç»†éœ€æ±‚
- è¯„ä¼°æŠ€æœ¯å¯è¡Œæ€§
- åˆ¶å®šè¯¦ç»†å®æ–½è®¡åˆ’
- å»ºç«‹é¡¹ç›®é‡Œç¨‹ç¢‘

å¦‚æœæ‚¨èƒ½æä¾›æ›´å¤šå…·ä½“çš„æŠ€æœ¯éœ€æ±‚æˆ–çº¦æŸæ¡ä»¶ï¼Œæˆ‘å¯ä»¥ç»™å‡ºæ›´æœ‰é’ˆå¯¹æ€§çš„å»ºè®®ã€‚æ‚¨å¸Œæœ›ä»å“ªä¸ªæ–¹é¢å¼€å§‹æ·±å…¥è®¨è®ºï¼Ÿ"""
    
    def _build_system_prompt(self, project_context: Optional[str] = None) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£å†³æŠ€æœ¯é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"""
        
        if project_context:
            base_prompt += f"\n\nå½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡ï¼š{project_context}\nè¯·åŸºäºé¡¹ç›®èƒŒæ™¯æä¾›ç›¸å…³å»ºè®®ã€‚"
        
        return base_prompt
    
    async def add_document_to_vector_db(
        self, 
        content: str, 
        metadata: Dict[str, Any],
        document_id: str,
        embedding_model_name: Optional[str] = None
    ) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            if not self.vector_db:
                logger.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
                return False
            
            # ä½¿ç”¨ç«å±±å¼•æ“è·å–åµŒå…¥å‘é‡
            embedding = volcengine_client.get_embedding(content)
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.vector_db.add(
                embeddings=[embedding],
                documents=[content],
                metadatas=[metadata],
                ids=[document_id]
            )
            
            logger.info(f"æ–‡æ¡£å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“: {document_id}")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    async def search_similar_documents(
        self, 
        query: str, 
        n_results: int = 5,
        embedding_model_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        try:
            if not self.vector_db:
                logger.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
                return []
            
            # ä½¿ç”¨ç«å±±å¼•æ“è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = volcengine_client.get_embedding(query)
            
            # æœç´¢ç›¸ä¼¼æ–‡æ¡£
            results = self.vector_db.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    formatted_results.append({
                        'content': doc,
                        'metadata': results['metadatas'][0][i] if results['metadatas'] and results['metadatas'][0] else {},
                        'distance': results['distances'][0][i] if results['distances'] and results['distances'][0] else 0.0
                    })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    async def process_project_files(self, project_id: str, file_paths: List[str]) -> bool:
        """å¤„ç†é¡¹ç›®æ–‡ä»¶"""
        try:
            for file_path in file_paths:
                content = await self._read_file_content(Path(file_path))
                if content:
                    metadata = {
                        "project_id": project_id,
                        "file_path": file_path,
                        "file_type": Path(file_path).suffix,
                        "processed_at": str(asyncio.get_event_loop().time())
                    }
                    
                    document_id = f"{project_id}_{Path(file_path).name}"
                    await self.add_document_to_vector_db(content, metadata, document_id)
            
            logger.info(f"é¡¹ç›®æ–‡ä»¶å¤„ç†å®Œæˆ: {project_id}")
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†é¡¹ç›®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    async def _read_file_content(self, file_path: Path) -> Optional[str]:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            if not file_path.exists():
                return None
                
            # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­å¤„ç†æ–¹å¼
            file_extension = file_path.suffix.lower()
            
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥è¯»å–
            if file_extension in ['.txt', '.md', '.csv', '.json', '.xml', '.html', '.htm']:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            
            # å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆPDFã€Wordç­‰ï¼‰ï¼Œä½¿ç”¨æ–‡ä»¶å·¥å…·æå–å†…å®¹
            elif file_extension in ['.pdf', '.doc', '.docx', '.xls', '.xlsx']:
                # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“é—¨çš„æ–‡ä»¶å†…å®¹æå–å·¥å…·
                # æš‚æ—¶è¿”å›æ–‡ä»¶ä¿¡æ¯è€Œä¸æ˜¯å†…å®¹
                return f"äºŒè¿›åˆ¶æ–‡ä»¶: {file_path.name}, å¤§å°: {file_path.stat().st_size} å­—èŠ‚"
            
            # å›¾ç‰‡æ–‡ä»¶
            elif file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']:
                return f"å›¾ç‰‡æ–‡ä»¶: {file_path.name}, å¤§å°: {file_path.stat().st_size} å­—èŠ‚"
            
            # å…¶ä»–æ–‡ä»¶å°è¯•æ–‡æœ¬è¯»å–
            else:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        return f.read()
                except:
                    return f"æ–‡ä»¶: {file_path.name}, å¤§å°: {file_path.stat().st_size} å­—èŠ‚"
                    
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "llm_model": volcengine_client.llm_model,
            "embedding_model": volcengine_client.embedding_model,
            "api_url": volcengine_client.base_url,
            "vector_db": "ChromaDB" if self.vector_db else "æœªåˆå§‹åŒ–"
        }

# åˆ›å»ºå…¨å±€AIæœåŠ¡å®ä¾‹
ai_service = AIService() 