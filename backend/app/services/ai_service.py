"""
AIæœåŠ¡æ¨¡å—
ä½¿ç”¨è±†åŒ…Embedding + FAISS + LangChainæœ€æ–°æ¶æ„ï¼Œæä¾›ç¨³å®šçš„RAGç³»ç»Ÿ
å‚è€ƒï¼šhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/
"""

import os
# ğŸ”§ è®¾ç½®OpenMPç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢FAISSç­‰åº“å†²çª - å¿…é¡»åœ¨AIåº“å¯¼å…¥ä¹‹å‰è®¾ç½®
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import json
import pickle
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator, Union
from pathlib import Path
from loguru import logger
import numpy as np
from pydantic import BaseModel

# LangChainæœ€æ–°å¯¼å…¥
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings

from .volcengine_client import volcengine_client
from ..core.model_config import model_manager

# é…ç½®æ—¥å¿—
logger.add("logs/ai_service.log", rotation="1 day", retention="7 days")

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯æ¨¡å‹"""
    role: str  # "user" æˆ– "assistant"
    content: str
    timestamp: Optional[str] = None

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    content: str
    sources: List[Dict[str, Any]] = []
    model: str
    usage: Dict[str, int] = {}

class DocumentSearchResult(BaseModel):
    """æ–‡æ¡£æœç´¢ç»“æœ"""
    document_id: str
    file_name: str
    content: str
    relevance_score: float
    metadata: Dict[str, Any] = {}

class VolcengineEmbeddings(Embeddings):
    """è±†åŒ…Embeddingæ¨¡å‹LangChainé€‚é…å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è±†åŒ…Embedding"""
        self.client = volcengine_client
        self.model = self.client.embedding_model
        logger.info(f"âœ… åˆå§‹åŒ–è±†åŒ…Embeddingæ¨¡å‹: {self.model}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        try:
            embeddings = []
            for text in texts:
                embedding = self.client.get_embedding(text)
                embeddings.append(embedding)
            logger.info(f"âœ… æˆåŠŸåµŒå…¥ {len(texts)} ä¸ªæ–‡æ¡£")
            return embeddings
        except Exception as e:
            logger.error(f"æ–‡æ¡£åµŒå…¥å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºé™çº§
            return [[0.0] * 2560 for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬"""
        try:
            embedding = self.client.get_embedding(text)
            logger.info(f"âœ… æˆåŠŸåµŒå…¥æŸ¥è¯¢æ–‡æœ¬")
            return embedding
        except Exception as e:
            logger.error(f"æŸ¥è¯¢åµŒå…¥å¤±è´¥: {e}")
            return [0.0] * 2560
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """å¼‚æ­¥æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self.embed_documents, texts
        )
    
    async def aembed_query(self, text: str) -> List[float]:
        """å¼‚æ­¥åµŒå…¥æŸ¥è¯¢æ–‡æœ¬"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self.embed_query, text
        )

class AIService:
    """AIæœåŠ¡ç±» - åŸºäºè±†åŒ…Embedding + FAISS + LangChainæœ€æ–°æ¶æ„"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        self.vector_store = None
        self.embeddings_model = None
        self.text_splitter = None
        self.documents_metadata = {}  # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        self.faiss_index_path = Path("../vector_storage")
        self.faiss_index_path.mkdir(exist_ok=True)
        
        self.initialize_models()
        self.initialize_vector_store()
    
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            # æµ‹è¯•ç«å±±å¼•æ“è¿æ¥
            if volcengine_client.test_connection():
                logger.info("âœ… ç«å±±å¼•æ“æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ ç«å±±å¼•æ“è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                
            # åˆå§‹åŒ–è±†åŒ…embeddingæ¨¡å‹
            try:
                logger.info("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–è±†åŒ…Embeddingæ¨¡å‹...")
                self.embeddings_model = VolcengineEmbeddings()
                logger.info("âœ… è±†åŒ…Embeddingæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as embed_error:
                logger.error(f"è±†åŒ…Embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {embed_error}")
                logger.warning("ä½¿ç”¨ç®€å•çš„æ–‡æœ¬åŒ¹é…ä½œä¸ºé™çº§æ–¹æ¡ˆ")
                self.embeddings_model = None
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ - ä½¿ç”¨æœ€æ–°å‚æ•°
            try:
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200,
                    length_function=len,
                    separators=["\n\n", "\n", " ", ""]
                )
                logger.info("âœ… æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as splitter_error:
                logger.error(f"æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å¤±è´¥: {splitter_error}")
                self.text_splitter = None
                
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # è®¾ç½®ä¸ºNoneï¼Œåç»­æ–¹æ³•ä¼šæ£€æŸ¥å¹¶ä½¿ç”¨é™çº§æ–¹æ¡ˆ
            self.embeddings_model = None
            self.text_splitter = None
    
    def initialize_vector_store(self):
        """åˆå§‹åŒ–FAISSå‘é‡å­˜å‚¨"""
        try:
            # å¦‚æœembeddingæ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å‘é‡å­˜å‚¨åˆå§‹åŒ–
            if not self.embeddings_model:
                logger.warning("âš ï¸ Embeddingæ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å‘é‡å­˜å‚¨åˆå§‹åŒ–")
                self.vector_store = None
                return
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„å‘é‡å­˜å‚¨
            faiss_file = self.faiss_index_path / "index.faiss"
            pkl_file = self.faiss_index_path / "index.pkl"
            metadata_file = self.faiss_index_path / "metadata.json"
            
            if faiss_file.exists() and pkl_file.exists():
                # åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨
                try:
                    self.vector_store = FAISS.load_local(
                        str(self.faiss_index_path), 
                        self.embeddings_model,
                        allow_dangerous_deserialization=True
                    )
                    
                    # åŠ è½½æ–‡æ¡£å…ƒæ•°æ®
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            self.documents_metadata = json.load(f)
                    
                    logger.info(f"âœ… ä»æœ¬åœ°åŠ è½½FAISSå‘é‡å­˜å‚¨æˆåŠŸ")
                    logger.info(f"  æ–‡æ¡£æ•°é‡: {len(self.documents_metadata)}")
                except Exception as load_error:
                    logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {load_error}")
                    # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                    self._create_new_vector_store()
            else:
                # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                self._create_new_vector_store()
            
            logger.info(f"å‘é‡å­˜å‚¨è·¯å¾„: {self.faiss_index_path}")
            
        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_store = None
    
    def _create_new_vector_store(self):
        """åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨"""
        try:
            # ç”¨ä¸€ä¸ªç©ºæ–‡æ¡£åˆå§‹åŒ–FAISS
            initial_doc = Document(page_content="åˆå§‹åŒ–æ–‡æ¡£", metadata={"type": "init"})
            self.vector_store = FAISS.from_documents([initial_doc], self.embeddings_model)
            logger.info("âœ… åˆ›å»ºæ–°çš„FAISSå‘é‡å­˜å‚¨")
        except Exception as e:
            logger.error(f"åˆ›å»ºå‘é‡å­˜å‚¨å¤±è´¥: {e}")
            self.vector_store = None
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°æœ¬åœ°"""
        try:
            if self.vector_store:
                self.vector_store.save_local(str(self.faiss_index_path))
                
                # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
                metadata_file = self.faiss_index_path / "metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(self.documents_metadata, f, ensure_ascii=False, indent=2)
                
                logger.info("âœ… å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°æœ¬åœ°")
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")
    
    async def add_document_to_vector_db(
        self, 
        content: str, 
        file_id: str, 
        file_name: str,
        project_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            if not self.vector_store or not self.text_splitter or not content.strip():
                logger.warning("å‘é‡å­˜å‚¨æˆ–æ–‡æœ¬åˆ†å‰²å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ–‡æ¡£æ·»åŠ ")
                return False
            
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_text(content)
            if not chunks:
                return False
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºDocumentå¯¹è±¡
            documents = []
            for i, chunk in enumerate(chunks):
                doc_metadata = {
                    "file_id": file_id,
                    "file_name": file_name,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "project_id": project_id or "default"
                }
                if metadata:
                    doc_metadata.update(metadata)
                
                documents.append(Document(page_content=chunk, metadata=doc_metadata))
            
            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
            self.vector_store.add_documents(documents)
            
            # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
            self.documents_metadata[file_id] = {
                "file_name": file_name,
                "project_id": project_id,
                "chunks_count": len(chunks),
                "metadata": metadata or {}
            }
            
            # å¼‚æ­¥ä¿å­˜
            await asyncio.get_event_loop().run_in_executor(None, self.save_vector_store)
            
            logger.info(f"âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“: {file_name} ({len(chunks)} chunks)")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    async def search_similar_documents(
        self, 
        query: str, 
        project_id: Optional[str] = None,
        top_k: int = 3
    ) -> List[DocumentSearchResult]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£ - ä½¿ç”¨è±†åŒ…Embedding"""
        try:
            if not self.vector_store or not query.strip():
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–æˆ–æŸ¥è¯¢ä¸ºç©º")
                return []
            
            # ä½¿ç”¨FAISSè¿›è¡Œç›¸ä¼¼åº¦æœç´¢
            docs_with_scores = self.vector_store.similarity_search_with_score(
                query, k=top_k * 2  # è·å–æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
            )
            
            results = []
            seen_files = set()
            
            for doc, score in docs_with_scores:
                # é¡¹ç›®è¿‡æ»¤
                if project_id and doc.metadata.get("project_id") != project_id:
                    continue
                
                file_id = doc.metadata.get("file_id", "unknown")
                file_name = doc.metadata.get("file_name", "Unknown")
                
                # é¿å…é‡å¤æ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶åªå–æœ€ç›¸å…³çš„chunk
                if file_id in seen_files:
                    continue
                seen_files.add(file_id)
                
                # è®¡ç®—ç›¸å…³æ€§åˆ†æ•° (FAISSè¿”å›çš„æ˜¯è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦)
                # è±†åŒ…embeddingä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè·ç¦»è¶Šå°ç›¸ä¼¼åº¦è¶Šé«˜
                relevance_score = max(0.0, 1.0 - score / 2.0)
                
                results.append(DocumentSearchResult(
                    document_id=file_id,
                    file_name=file_name,
                    content=doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                    relevance_score=relevance_score,
                    metadata=doc.metadata
                ))
                
                if len(results) >= top_k:
                    break
            
            # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
            results.sort(key=lambda x: x.relevance_score, reverse=True)
            
            logger.info(f"ğŸ” æœç´¢æŸ¥è¯¢: {query[:50]}...")
            logger.info(f"ğŸ“„ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢ç›¸ä¼¼æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        messages: List[ChatMessage], 
        project_context: Optional[str] = None,
        stream: bool = False,
        model_name: Optional[str] = None
    ) -> ChatResponse:
        """èŠå¤©å®Œæˆ"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(project_context)
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            api_messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            for msg in messages:
                # å¤„ç†å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼çš„æ¶ˆæ¯
                if isinstance(msg, dict):
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                else:
                    role = getattr(msg, "role", "user")
                    content = getattr(msg, "content", "")
                    
                api_messages.append({
                    "role": role,
                    "content": content
                })
            
            # ä½¿ç”¨ç«å±±å¼•æ“ç”Ÿæˆå›å¤
            response_content = volcengine_client.chat_completion(api_messages)
            
            return ChatResponse(
                content=response_content,
                model=volcengine_client.llm_model,
                usage={
                    "prompt_tokens": len(str(api_messages)),
                    "completion_tokens": len(response_content),
                    "total_tokens": len(str(api_messages)) + len(response_content)
                }
            )
                
        except Exception as e:
            logger.error(f"èŠå¤©å®Œæˆå¤±è´¥: {e}")
            return ChatResponse(
                content="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                model="error",
                usage={}
            )
    
    async def chat_completion_stream(
        self, 
        messages: List[ChatMessage], 
        project_context: Optional[str] = None,
        model_name: Optional[str] = None
    ):
        """èŠå¤©å®Œæˆ - æµå¼å“åº”"""
        try:
            logger.info(f"ğŸ”¥ å¼€å§‹æµå¼èŠå¤©å®Œæˆï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
            logger.info(f"ğŸ”¥ æ¶ˆæ¯ç±»å‹: {[type(msg).__name__ for msg in messages]}")
            logger.info(f"ğŸ”¥ å‰3æ¡æ¶ˆæ¯å†…å®¹: {messages[:3] if len(messages) <= 3 else messages[:3]}")
            
            # ğŸ¤– æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼ºï¼šæœç´¢ç›¸å…³é¡¹ç›®æ–‡æ¡£
            enhanced_context = await self._build_enhanced_context(messages, project_context)
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(enhanced_context)
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            api_messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            for msg in messages:
                # å¤„ç†å­—å…¸æ ¼å¼å’Œå¯¹è±¡æ ¼å¼çš„æ¶ˆæ¯
                if isinstance(msg, dict):
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                else:
                    role = getattr(msg, "role", "user")
                    content = getattr(msg, "content", "")
                    
                api_messages.append({
                    "role": role,
                    "content": content
                })
            
            # å°è¯•ä½¿ç”¨çœŸå®çš„æµå¼AIæœåŠ¡
            try:
                # è¿™é‡Œå¯ä»¥æ¥å…¥çœŸå®çš„æµå¼AIæœåŠ¡
                # ç›®å‰ä½¿ç”¨æ¨¡æ‹Ÿçš„æµå¼å“åº”
                response_content = volcengine_client.chat_completion(api_messages)
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º - å°†å®Œæ•´å›å¤æŒ‰å­—ç¬¦åˆ†å‰²
                import asyncio
                words = response_content.split()
                for i, word in enumerate(words):
                    if i == 0:
                        yield word
                    else:
                        yield f" {word}"
                    await asyncio.sleep(0.1)  # æ§åˆ¶è¾“å‡ºé€Ÿåº¦
                    
            except Exception as ai_error:
                logger.warning(f"AIæœåŠ¡æµå¼è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: {ai_error}")
                # é™çº§å¤„ç† - ç”Ÿæˆæ™ºèƒ½å›å¤å¹¶æµå¼è¾“å‡º
                last_message_content = ""
                if messages:
                    last_msg = messages[-1]
                    if isinstance(last_msg, dict):
                        last_message_content = last_msg.get("content", "")
                    else:
                        last_message_content = getattr(last_msg, "content", "")
                        
                fallback_response = self._generate_fallback_response(
                    last_message_content, 
                    project_context
                )
                
                words = fallback_response.split()
                for i, word in enumerate(words):
                    if i == 0:
                        yield word
                    else:
                        yield f" {word}"
                    await asyncio.sleep(0.08)  # ç¨å¿«ä¸€äº›çš„è¾“å‡ºé€Ÿåº¦
                    
        except Exception as e:
            logger.error(f"æµå¼èŠå¤©å®Œæˆå¤±è´¥: {e}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"å®Œæ•´å †æ ˆ: {traceback.format_exc()}")
            
            # å‘é€é”™è¯¯ä¿¡æ¯ - ä½†å®é™…ä¸Šåº”è¯¥å‘é€æ­£å¸¸å›å¤
            # ä¸´æ—¶ä¿®å¤ï¼šç›´æ¥å‘é€ä¸€ä¸ªå‹å¥½çš„å›å¤
            friendly_response = "ä½ å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ã€‚è™½ç„¶å½“å‰é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œä½†æˆ‘å¾ˆä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥ä¸ºæ‚¨åšçš„å—ï¼Ÿ"
            
            words = friendly_response.split()
            for i, word in enumerate(words):
                if i == 0:
                    yield word
                else:
                    yield f" {word}"
                import asyncio
                await asyncio.sleep(0.1)
    
    async def _build_enhanced_context(self, messages: List[ChatMessage], project_context: Optional[str] = None) -> str:
        """æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡ï¼šåŸºäºç”¨æˆ·æ¶ˆæ¯æœç´¢ç›¸å…³é¡¹ç›®æ–‡æ¡£"""
        try:
            if not messages:
                return project_context or ""
            
            # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ç”¨äºæœç´¢
            last_message = messages[-1] if messages else None
            if not last_message:
                return project_context or ""
            
            # æå–æŸ¥è¯¢å†…å®¹
            if isinstance(last_message, dict):
                query = last_message.get("content", "")
            else:
                query = getattr(last_message, "content", "")
            
            if not query.strip():
                return project_context or ""
            
            logger.info(f"ğŸ¤– å¼€å§‹æ™ºèƒ½ä¸Šä¸‹æ–‡æœç´¢ï¼ŒæŸ¥è¯¢: {query[:100]}...")
            
            # ä»project_contextä¸­æå–é¡¹ç›®IDï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            project_id = project_context if project_context and project_context.startswith("project-") else None
            
            # æœç´¢ç›¸å…³æ–‡æ¡£
            relevant_docs = await self.search_similar_documents(
                query=query,
                project_id=project_id
            )
            
            # æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡
            context_parts = []
            
            if project_context:
                context_parts.append(f"é¡¹ç›®èƒŒæ™¯: {project_context}")
            
            if relevant_docs:
                context_parts.append("ğŸ“š ç›¸å…³é¡¹ç›®æ–‡æ¡£:")
                for i, doc in enumerate(relevant_docs[:3], 1):
                    file_name = doc.file_name
                    content_preview = doc.content[:200] + ("..." if len(doc.content) > 200 else "")
                    relevance = doc.relevance_score
                    
                    context_parts.append(f"""
{i}. æ–‡ä»¶: {file_name} (ç›¸å…³æ€§: {relevance:.2f})
   å†…å®¹æ‘˜è¦: {content_preview}""")
                
                logger.info(f"ğŸ¤– æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œå·²æ·»åŠ åˆ°ä¸Šä¸‹æ–‡")
            else:
                logger.info("ğŸ¤– æœªæ‰¾åˆ°ç›¸å…³é¡¹ç›®æ–‡æ¡£")
            
            enhanced_context = "\n".join(context_parts)
            return enhanced_context
            
        except Exception as e:
            logger.error(f"æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return project_context or ""
    
    def _generate_fallback_response(self, user_input: str, project_context: Optional[str] = None) -> str:
        """ç”Ÿæˆé™çº§å›å¤"""
        if not user_input:
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"
        
        # æ ¹æ®å…³é”®è¯ç”Ÿæˆç›¸å…³å›å¤
        user_input_lower = user_input.lower()
        
        if any(keyword in user_input_lower for keyword in ['æ¶æ„', 'è®¾è®¡', 'ç³»ç»Ÿ']):
            return """å…³äºç³»ç»Ÿæ¶æ„è®¾è®¡ï¼Œæˆ‘å»ºè®®è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªå…³é”®æ–¹é¢ï¼š

ğŸ—ï¸ **æ¶æ„è®¾è®¡åŸåˆ™ï¼š**
1. **æ¨¡å—åŒ–è®¾è®¡** - å°†ç³»ç»Ÿåˆ†è§£ä¸ºç‹¬ç«‹çš„æ¨¡å—
2. **å¯æ‰©å±•æ€§** - æ”¯æŒæœªæ¥åŠŸèƒ½æ‰©å±•
3. **é«˜å¯ç”¨æ€§** - ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
4. **æ€§èƒ½ä¼˜åŒ–** - å…³æ³¨å“åº”æ—¶é—´å’Œååé‡

ğŸ’¡ **æŠ€æœ¯é€‰å‹å»ºè®®ï¼š**
- é€‰æ‹©æˆç†Ÿç¨³å®šçš„æŠ€æœ¯æ ˆ
- è€ƒè™‘å›¢é˜ŸæŠ€æœ¯æ ˆç†Ÿæ‚‰åº¦
- è¯„ä¼°æŠ€æœ¯çš„é•¿æœŸç»´æŠ¤æ€§
- æƒè¡¡å¼€å‘æ•ˆç‡å’Œæ€§èƒ½éœ€æ±‚

æ‚¨å¸Œæœ›æ·±å…¥è®¨è®ºå“ªä¸ªå…·ä½“çš„æ¶æ„æ–¹é¢ï¼Ÿ"""

        elif any(keyword in user_input_lower for keyword in ['æ•°æ®', 'æ•°æ®åº“', 'å­˜å‚¨']):
            return """å…³äºæ•°æ®ç®¡ç†å’Œå­˜å‚¨ï¼Œè¿™é‡Œæœ‰ä¸€äº›ä¸“ä¸šå»ºè®®ï¼š

ğŸ“Š **æ•°æ®å­˜å‚¨ç­–ç•¥ï¼š**
1. **å…³ç³»å‹æ•°æ®åº“** - é€‚åˆç»“æ„åŒ–æ•°æ®å’Œäº‹åŠ¡å¤„ç†
2. **éå…³ç³»å‹æ•°æ®åº“** - é€‚åˆå¤§è§„æ¨¡æ•°æ®å’Œçµæ´»schema
3. **ç¼“å­˜ç³»ç»Ÿ** - æå‡æ•°æ®è®¿é—®æ€§èƒ½
4. **æ•°æ®å¤‡ä»½** - ç¡®ä¿æ•°æ®å®‰å…¨æ€§

ğŸ” **æ•°æ®å¤„ç†æœ€ä½³å®è·µï¼š**
- æ•°æ®æ¸…æ´—å’ŒéªŒè¯
- å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§
- å®æ–½æ•°æ®å®‰å…¨ç­–ç•¥
- ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½

éœ€è¦é’ˆå¯¹ç‰¹å®šçš„æ•°æ®åœºæ™¯è¿›è¡Œæ·±å…¥åˆ†æå—ï¼Ÿ"""

        elif any(keyword in user_input_lower for keyword in ['AI', 'æœºå™¨å­¦ä¹ ', 'ç®—æ³•', 'æ¨¡å‹']):
            return """å…³äºAIå’Œæœºå™¨å­¦ä¹ å®æ–½ï¼Œæˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹æŒ‡å¯¼ï¼š

ğŸ¤– **AIé¡¹ç›®å¼€å‘æµç¨‹ï¼š**
1. **éœ€æ±‚åˆ†æ** - æ˜ç¡®AIåº”ç”¨åœºæ™¯
2. **æ•°æ®å‡†å¤‡** - æ”¶é›†å’Œé¢„å¤„ç†è®­ç»ƒæ•°æ®
3. **æ¨¡å‹é€‰æ‹©** - æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ç®—æ³•
4. **æ¨¡å‹è®­ç»ƒ** - ä½¿ç”¨åˆé€‚çš„è®­ç»ƒç­–ç•¥
5. **æ¨¡å‹è¯„ä¼°** - å¤šç»´åº¦è¯„ä¼°æ¨¡å‹æ€§èƒ½
6. **éƒ¨ç½²ä¸Šçº¿** - å°†æ¨¡å‹é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒ

âš¡ **å…³é”®æŠ€æœ¯è¦ç‚¹ï¼š**
- ç‰¹å¾å·¥ç¨‹çš„é‡è¦æ€§
- è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„å¤„ç†
- æ¨¡å‹è§£é‡Šæ€§å’Œå¯ä¿¡åº¦
- æŒç»­å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°

æ‚¨æƒ³äº†è§£å“ªä¸ªå…·ä½“çš„AIæŠ€æœ¯ç»†èŠ‚ï¼Ÿ"""

        else:
            # é€šç”¨å›å¤
            context_info = f"åœ¨{project_context}çš„èƒŒæ™¯ä¸‹ï¼Œ" if project_context else ""
            return f"""æ„Ÿè°¢æ‚¨çš„æé—®ï¼{context_info}æˆ‘ä¸ºæ‚¨æä¾›ä»¥ä¸‹åˆ†æï¼š

ğŸ’¡ **é—®é¢˜ç†è§£ï¼š**
æ‚¨è¯¢é—®çš„"{user_input}"æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚

ğŸ¯ **å»ºè®®æ–¹å‘ï¼š**
1. **æ·±å…¥åˆ†æ** - ä»å¤šä¸ªè§’åº¦ç†è§£é—®é¢˜æœ¬è´¨
2. **æœ€ä½³å®è·µ** - å‚è€ƒè¡Œä¸šæ ‡å‡†å’ŒæˆåŠŸæ¡ˆä¾‹
3. **é£é™©è¯„ä¼°** - è¯†åˆ«æ½œåœ¨çš„æŠ€æœ¯å’Œä¸šåŠ¡é£é™©
4. **å®æ–½ç­–ç•¥** - åˆ¶å®šå¾ªåºæ¸è¿›çš„è§£å†³æ–¹æ¡ˆ

ğŸš€ **ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š**
- æ”¶é›†æ›´å¤šè¯¦ç»†éœ€æ±‚
- è¯„ä¼°æŠ€æœ¯å¯è¡Œæ€§
- åˆ¶å®šè¯¦ç»†å®æ–½è®¡åˆ’
- å»ºç«‹é¡¹ç›®é‡Œç¨‹ç¢‘

å¦‚æœæ‚¨èƒ½æä¾›æ›´å¤šå…·ä½“çš„æŠ€æœ¯éœ€æ±‚æˆ–çº¦æŸæ¡ä»¶ï¼Œæˆ‘å¯ä»¥ç»™å‡ºæ›´æœ‰é’ˆå¯¹æ€§çš„å»ºè®®ã€‚æ‚¨å¸Œæœ›ä»å“ªä¸ªæ–¹é¢å¼€å§‹æ·±å…¥è®¨è®ºï¼Ÿ"""
    
    def _build_system_prompt(self, project_context: Optional[str] = None) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIé¡¹ç›®ç®¡ç†åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

ğŸ¯ **ä¸“ä¸šé¢†åŸŸ**ï¼š
- AI/æœºå™¨å­¦ä¹ é¡¹ç›®ç®¡ç†
- è½¯ä»¶å¼€å‘æ¶æ„è®¾è®¡
- æ•°æ®ç§‘å­¦é¡¹ç›®å®æ–½
- æŠ€æœ¯é—®é¢˜è§£å†³å’Œå’¨è¯¢

ğŸ’¡ **å›ç­”åŸåˆ™**ï¼š
- åŸºäºé¡¹ç›®å®é™…æƒ…å†µæä¾›ä¸“ä¸šå»ºè®®
- ç»“åˆæœ€ä½³å®è·µå’Œè¡Œä¸šæ ‡å‡†
- æä¾›å¯æ“ä½œçš„è§£å†³æ–¹æ¡ˆ
- ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§

ğŸ“š **ä¸Šä¸‹æ–‡ç†è§£**ï¼š
- ä¼šåˆ†æé¡¹ç›®é˜¶æ®µå’Œå…·ä½“éœ€æ±‚
- åŸºäºå·²æœ‰æ–‡æ¡£å’Œèµ„æ–™æä¾›å»ºè®®
- è€ƒè™‘é¡¹ç›®çš„æŠ€æœ¯æ ˆå’Œçº¦æŸæ¡ä»¶"""
        
        if project_context:
            base_prompt += f"""

ğŸ” **å½“å‰é¡¹ç›®ä¸Šä¸‹æ–‡**ï¼š{project_context}

è¯·åŸºäºä»¥ä¸Šé¡¹ç›®èƒŒæ™¯ï¼Œç»“åˆä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›æœ‰é’ˆå¯¹æ€§çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆã€‚å¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠå…·ä½“çš„æŠ€æœ¯ç»†èŠ‚ï¼Œè¯·å°½å¯èƒ½æä¾›è¯¦ç»†çš„å®æ–½æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ã€‚"""
        
        return base_prompt
    
    async def process_project_files(self, project_id: str, file_paths: List[str]) -> bool:
        """å¤„ç†é¡¹ç›®æ–‡ä»¶"""
        try:
            for file_path in file_paths:
                content = await self._read_file_content(Path(file_path))
                if content:
                    metadata = {
                        "project_id": project_id,
                        "file_path": file_path,
                        "file_type": Path(file_path).suffix,
                        "processed_at": str(asyncio.get_event_loop().time())
                    }
                    
                    document_id = f"{project_id}_{Path(file_path).name}"
                    await self.add_document_to_vector_db(content, document_id, Path(file_path).name, project_id, metadata)
            
            logger.info(f"é¡¹ç›®æ–‡ä»¶å¤„ç†å®Œæˆ: {project_id}")
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†é¡¹ç›®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    async def _read_file_content(self, file_path: Path) -> Optional[str]:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            if not file_path.exists():
                return None
                
            # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­å¤„ç†æ–¹å¼
            file_extension = file_path.suffix.lower()
            
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥è¯»å–
            if file_extension in ['.txt', '.md', '.csv', '.json', '.xml', '.html', '.htm']:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            
            # å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆPDFã€Wordç­‰ï¼‰ï¼Œä½¿ç”¨æ–‡ä»¶å·¥å…·æå–å†…å®¹
            elif file_extension in ['.pdf', '.doc', '.docx', '.xls', '.xlsx']:
                # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä¸“é—¨çš„æ–‡ä»¶å†…å®¹æå–å·¥å…·
                # æš‚æ—¶è¿”å›æ–‡ä»¶ä¿¡æ¯è€Œä¸æ˜¯å†…å®¹
                return f"äºŒè¿›åˆ¶æ–‡ä»¶: {file_path.name}, å¤§å°: {file_path.stat().st_size} å­—èŠ‚"
            
            # å›¾ç‰‡æ–‡ä»¶
            elif file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']:
                return f"å›¾ç‰‡æ–‡ä»¶: {file_path.name}, å¤§å°: {file_path.stat().st_size} å­—èŠ‚"
            
            # å…¶ä»–æ–‡ä»¶å°è¯•æ–‡æœ¬è¯»å–
            else:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        return f.read()
                except:
                    return f"æ–‡ä»¶: {file_path.name}, å¤§å°: {file_path.stat().st_size} å­—èŠ‚"
                    
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "llm_model": volcengine_client.llm_model,
            "embedding_model": volcengine_client.embedding_model,
            "api_url": volcengine_client.base_url,
            "vector_db": "FAISS + è±†åŒ…Embedding" if self.vector_store else "æœªåˆå§‹åŒ–"
        }

# åˆ›å»ºå…¨å±€AIæœåŠ¡å®ä¾‹
ai_service = AIService() 